---
layout: cover
---

## Dask Cluster Types and Deployment

### Session 3: 11:15 AM - 12:00 PM

---


## Clusters versus Runners


**Clusters:** Creates new jobs for Dask workers. Scales to the number of workers needed, if available.
- Dynamic scaling of workers by submitting new jobs.
- Requires job queue permissions
- Better for interactive/adaptive workloads


**Runners:** Uses existing allocated resources to run Dask workers. Does not scale beyond the initial allocation.
- Uses existing resources without submitting new jobs.
- Sets up scheduler on head node, workers on remaining nodes

**Key Features:**
- Dask automatically discovers available nodes from SLURM environment
- Dask handles network configuration and port management


---

## Dask Cluster Types
### HPC Clusters
**Multi-Machine Parallelism**
- SLURM, PBS, LSF, SGE support
- Submit Dask workers as batch jobs
- Automatic scaling based on workload

```python
from dask_jobqueue import SLURMCluster
cluster = SLURMCluster(
    queue='normal',
    cores=24,
    memory='128GB',
    walltime='02:00:00',
    job_extra=['--constraint=haswell']
)

cluster.scale(jobs=10)  # Request 10 nodes
client = Client(cluster)
```

---


## Dask Cluster Types

### Cloud-Native Solutions


**Kubernetes Clusters**
- Container orchestration for Dask workers
- Auto-scaling based on demand
- Integration with cloud providers (AWS, GCP, Azure)

```python
from dask_kubernetes import KubeCluster
cluster = KubeCluster.from_yaml('worker-spec.yaml')
cluster.scale(20)  # 20 worker pods
client = Client(cluster)

```

**Coiled:** Managed Dask clusters with auto-scaling and easy deployment.

**Saturn Cloud:** Provides Dask clusters with Jupyter integration and easy scaling.

**AWS Fargate:** Serverless Dask clusters on AWS, automatically scaling based on workload.

---


## Leveraging BioHPC for Dask


### How to perform a multi-node SlurmRunner job on BioHPC


sbatch script example:

```bash
#!/bin/bash
#SBATCH --job-name dask-seg
#SBATCH -p 512GB
#SBATCH -N 6
#SBATCH --mem 501760 
#SBATCH -t 2-0:0:00
#SBATCH -o /home2/kdean/portal_jobs/job_%j.out
#SBATCH -e /home2/kdean/portal_jobs/job_%j.err
#SBATCH --mail-type ALL
#SBATCH --mail-user kevin.dean@UTSouthwestern.edu

export PATH="/project/bioinformatics/Danuser_lab/Dean/dean/miniconda3/bin:$PATH"
eval "$(/project/bioinformatics/Danuser_lab/Dean/dean/miniconda3/bin/conda shell.bash hook)"
conda --version
source activate u_Segment3D_env

srun -n 5 \
   python /archive/bioinformatics/Danuser_lab/Dean/dean/git/data_analysis/segmentation/distributed_dog.py \ 
   --job-id "${SLURM_JOB_ID}"
exit 0


```


---


## Leveraging BioHPC for Dask

### How to perform a multi-node SlurmRunner job on BioHPC (continued)

**Imports**

```python
# Standard Library Imports
import os
import argparse
import logging

# Third Party Imports
import numpy as np
from scipy import ndimage
import skimage.filters as filters
import skimage.measure as measure
import zarr
from dask.distributed import Client
from dask_jobqueue.slurm import SLURMRunner
import dask.array as da

...
```

---


## Leveraging BioHPC for Dask
### How to perform a multi-node SlurmRunner job on BioHPC (continued)

**Setting up Logging**
```python
parser = argparse.ArgumentParser()
parser.add_argument("--job-id", type=str, required=True, help="SLURM Job ID")
args = parser.parse_args()
log_path = os.path.join("/home2/kdean/portal_jobs/", f"job_{args.job_id}.log")
logging.basicConfig(
    filename=log_path,
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

logging.getLogger().setLevel(logging.INFO)
logging.info(f"SLURM Job ID: {args.job_id}")

...
```

_Provides a way to log messages to a file, which can be useful for debugging and tracking the progress of the job._

---



## Leveraging BioHPC for Dask


### How to perform a multi-node SlurmRunner job on BioHPC (continued)

**Setting up SLURM Runner**

```python
with SLURMRunner(scheduler_file=schedule_path) as runner:
    with Client(runner) as client:     
        client.wait_for_workers(runner.n_workers)
        client.run(lambda: logging.basicConfig(level=logging.INFO))
        logging.info(f"Dask Client connected to the scheduler: {client}")
        logging.info(f"Number of workers: {len(client.scheduler_info()['workers'])}")

        # Load the Zarr file with Dask
        data_path = os.path.join(base_path, 'output.zarr')
        dask_data = da.from_zarr(data_path)
        data_shape = dask_data.shape # (840, 3, 413, 2048, 2048)
        logging.info(f"Loaded data: {data_path}")

        # Saving the data to a Zarr file.
        save_path = os.path.join(base_path, 'glomeruli.zarr')
        zarr_store = zarr.open(save_path, mode='a',
            shape=(data_shape[0], data_shape[2], data_shape[3], data_shape[4]),
            dtype='uint16')

        ...
```


---

## Leveraging BioHPC for Dask


### How to perform a multi-node SlurmRunner job on BioHPC (continued)


**Running the Difference of Gaussian (DoG) filter on the data:**

```python
        segmentation_channel = 2
        high_pass = 50
        low_pass = 200
        for position in reversed(range(data_shape[0]-1)):
            image = dask_data[position, segmentation_channel, :, :, :].astype(np.float32)
            logging.info(f"Processing position {position}.")

            # Difference of Gaussian Filtering.
            high_pass_filtered = image.map_overlap(
                ndimage.gaussian_filter, sigma=high_pass/2.35, order=0, mode="nearest", depth=40)

            low_pass_filtered = image.map_overlap(
                ndimage.gaussian_filter, sigma=low_pass/2.35, order=0, mode="nearest", depth=40)

            dog_filtered = da.map_blocks(
                np.subtract, high_pass_filtered, low_pass_filtered).compute()

            # Save the image as a Zarr file.
            zarr_store[position, :, :, :] = dog_filtered

```


---

## Leveraging BioHPC for Dask
### How to perform a multi-node SlurmCluster job on BioHPC

**Slurm Clusters**
- Use `SLURMCluster` to create a Dask cluster that submits jobs to SLURM.
- Run a single python script that initializes the cluster and submits jobs. Can be submitted via 'sbatch' command, or via a WebVisualization job.
- Cluster configuration directly specified in script, or can be loaded from a YAML file.

```python
cluster_kwargs = {
        'cluster_type': 'lancaster_slurm_cluster',
        'cores': 40, # Number of threads per worker (utilizing cores within each process)
        'death_timeout': "600s",
        'interface': 'ib0', # Ensures internal worker scheduler comms use ib0
        'job_extra_directives': [
            "--nodes=1", # Each worker uses one node.
            "--ntasks=1", # Number of tasks each job launches.
            "--mail-type FAIL",
            "--mail-user kevin.dean@utsouthwestern.edu",
            "-o job_%j.out",
            "-e job_%j.err"
        ],
```

---

## Leveraging BioHPC for Dask
### How to perform a multi-node SlurmCluster job on BioHPC (continued)

```python
        'job_name': "multinode_warp",
        'memory': '220GB',
        'min_workers': 2,
        'max_workers': 16,
        'n_workers': 16, 
        'local_directory': "/archive/bioinformatics/Danuser_lab/Dean/dean/temp",
        'processes': 1, # Number of Python processes/worker.
        'scheduler_options': {
            "host": "10.100.161.251",
            "dashboard_address": ":8788",
            "dashboard": {
                "session_token_expiration": 3600
            }
        },
        'queue': "256GB",       # 32GB, 128GB, 512GB Queue/partition name
        'walltime': "36:00:00",
    }
```

---

## Cluster Management Best Practices


**Resource Planning**
- Worker memory vs chunk size relationships 
- Network bandwidth considerations 
- Storage locality optimization

**Monitoring and Debugging** 
- Dask dashboard for real-time monitoring
- Performance profiling tools 
- Common bottlenecks and solutions

```python
# Monitor Cluster Performance
client.dashboard_link # web link to Dask dashboard
client.get_worker_logs() # retrieve logs from workers

```
---
layout: cover

---
## Introduction to Dask and Parallel Computing 
### Session 1: 9:00 - 10:00 AM

---

## Big Data Challenges

<v-clicks>

- **Data Volume:** Modern datasets (microscopy images, genomics); often exceed a single machine’s RAM, making traditional in-memory tools (e.g., Pandas); impractical. Even data close to memory size can be unwieldy due to copies during processing.
- **CPU Limitations:** Python’s Global Interpreter Lock (GIL) means pure Python code runs on only one core, so typical NumPy/Pandas workflows run single-threaded and fail to use modern multi-core CPUs. This leaves significant performance potential untapped on multi-core machines.
- **Need for Parallelism:** To handle big data within reasonable time, we must go _out-of-core_ (process data in chunks from disk) and exploit parallel computing—using all CPU cores or multiple cluster nodes in tandem. This requires tools beyond the standard single-threaded Python approach (e.g., multi-processing or distributed computing frameworks).

</v-clicks>

---
layout: image-right
image: pages/images/dask_array.png
backgroundSize: 60%
---

## Dask Overview - Out-of-Core & Lazy Execution

**Blocked Algorithms**: Dask breaks up large data into many small pieces (chunks) that fit in memory. This enables computations on datasets larger than RAM by processing each chunk sequentially or in parallel (out-of-core execution). 

**Lazy Evaluation**: Operations on Dask collections are _lazy_ – they build a task graph of work to be done, but no actual computation happens until you explicitly call `.compute()` or similar to execute the graph. This avoids needless work and allows optimization before running tasks.

---

## Dask Overview - Out-of-Core & Lazy Execution


**Task Graph & Scheduler**: Each Dask computation creates a directed acyclic graph (DAG) of tasks. When triggered, Dask’s _scheduler_ orchestrates executing these tasks on parallel hardware (threads, processes, or cluster workers) to produce the result. This separation of the task graph from execution allows Dask to scale transparently. 

**Dask Execution Flow**: User Collections (e.g. Dask DataFrame) create a Task Graph of many small tasks, which a _Scheduler_ executes on available resources. This design lets users write high-level code while Dask handles parallel execution. (Dask offers both a single-machine scheduler and a distributed scheduler for clusters.)

---
layout: image-left
image: pages/images/map-reduce-task-scheduling.svg
backgroundSize: 90%
---

## What is a Directed Acyclic Graph (DAG)?

A Directed Acyclic Graph (DAG) is a graph structure with directed edges and no cycles. In Dask, each node represents a task, and edges represent dependencies between tasks.

A task is a unit of work (e.g., a function call), and dependencies indicate which tasks must complete before others can start.

Tasks are automatically scheduled based upon their dependencies, allowing Dask to execute them in parallel where possible.

---

## Parallelism on a Single Machine

<v-clicks>

- Multi-Threading: Dask’s default scheduler on a single machine uses a thread pool to execute tasks concurrently. This incurs very little overhead (tasks run in the same process), and works well when computations release the GIL (e.g. NumPy ufuncs, Pandas C code). Due to Python’s GIL, pure Python code cannot run on multiple threads at the same time, so multi-threading yields speedups mainly for numeric or I/O-bound workloads. 

- Multi-Processing: For Python-heavy workloads, Dask can use multiple processes. The multiprocessing scheduler runs tasks in separate Python processes, bypassing the GIL to achieve true parallelism. This allows parallel execution even for code that doesn’t release the GIL (e.g. processing text or Python objects). The trade-off is overhead: transferring data between processes can be costly, especially if large data needs to be serialized. 

</v-clicks>

<v-clicks>

**Choosing a Scheduler:** Dask chooses a sensible default (threads for arrays/dataframes, processes for bags) but you can configure the scheduler. For compute-heavy pure Python tasks, using processes (or the distributed scheduler in local mode) can improve performance. For numeric array/dataframe tasks, threads are often effective since NumPy/Pandas do the heavy lifting in C/C++ (no GIL contention).

</v-clicks>

---

## Dask's Scheduler System

<v-clicks>

- Advanced Local Scheduler: Dask’s distributed scheduler can run on a single machine or across a cluster. Even on one machine, many users prefer the distributed scheduler for its richer features: an asynchronous Future API, a live diagnostic dashboard, and smarter task scheduling that can outperform the simple multiprocessing approach. You can start a local distributed scheduler with a few lines of code and get insight into task execution in real time.
- Scaling to Clusters: The distributed scheduler truly shines when scaling out to multiple machines. It can coordinate a cluster of worker processes spread across several nodes, handling communication and load balancing. Dask can integrate with HPC job schedulers (SLURM, PBS, LSF, etc.) to launch workers on a cluster. This means you can write your code using Dask’s API and then deploy it on an HPC cluster, harnessing hundreds of cores or more.
</v-clicks>

---

## Dask's Scheduler System

**Same Code, Larger Scale...** 

- Thanks to Dask’s abstraction, code written for Dask on your laptop can run on a cluster with minimal or no changes. 
- The scheduler and cluster take care of distributing data and computations. 
- This lets you prototype locally and then scale out to big data on HPC when needed – a key benefit for scientific computing workflows.

```python
from dask.distributed import Client, LocalCluster
# Local cluster with custom configuration
cluster = LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='2GB')
client = Client(cluster)

```
--- 

## Dask Collections (Parallel Data Structures)

Dask provides high-level collections that mimic familiar APIs, allowing you to scale up workflows with minimal code changes:

<v-clicks>

- Dask Array: Parallel N-dimensional array (NumPy-like). Useful for large numerical data (e.g. large images, multi-dim arrays). 
- Dask DataFrame: Parallel DataFrame (Pandas-like). For large tabular datasets (e.g. CSVs, data frames) split into many partitions. 
- Dask Bag: Parallel collection for unstructured data (like a list of Python objects, text logs, JSON records). It’s a general-purpose container. 
- Dask Delayed: A low-level way to build custom task graphs. You wrap arbitrary Python functions with dask.delayed to create tasks and dependencies manually.

</v-clicks>

<v-clicks>

All these collections run on the same Dask scheduler infrastructure. They mirror NumPy/Pandas APIs closely, so you can often switch to Dask by changing imports and adding a `.compute()` call.

</v-clicks>

---

## Minimal Code Changes: Pandas vs Dask DataFrame

For example, switching from Pandas to Dask DataFrame in an analysis is straightforward ￼ – usually just change the import, possibly load data in partitions, and call .compute() when needed:

```python
# Pandas usage (single-machine, in-memory)
import pandas as pd
df = pd.read_csv("data.csv")
result = df.groupby('category')['value'].mean()

# Dask usage (parallel, can be larger-than-memory)
import dask.dataframe as dd
df = dd.read_csv("data_*.csv")        # can read multiple files in chunks
result = df.groupby('category')['value'].mean().compute()
```

Both snippets above do the same group-by operation. The Dask version spreads the work across many partitions and CPUs, then uses .compute() to get the final result.

<!--
Presentation comments, should we want them.
-->


---

## Minimal Code Changes: NumPy vs Dask Array

For NumPy arrays, switching to Dask Array is similarly straightforward:

```python
# NumPy usage (single-machine, in-memory)
import numpy as np
arr = np.random.rand(10000, 10000)  # large array
result = np.mean(arr, axis=0)

# Dask usage (parallel, can be larger-than-memory)
import dask.array as da
arr = da.random.random((10000, 10000), chunks=(1000, 1000))  # large array in chunks
result = da.mean(arr, axis=0).compute()  # compute mean across chunks
```

Both snippets perform the same mean operation on a large array. The Dask version processes the data in chunks, allowing it to handle arrays larger than memory and utilize all CPU cores. 

**_Exercise 0._**

---

## Demo: Parallelizing with Dask Delayed

Here two independent tasks can run in parallel, then feed into an add task which combines their results.

```python
import numpy as np
from dask import delayed
from dask.distributed import Client

client = Client()  # Launch local cluster of workers

# Two large arrays (e.g. parts of a dataset)
A, B = np.random.random((10000, 10000)), np.random.random((10000, 10000))

# Create delayed tasks for summing each array
sumA, sumB = delayed(np.sum)(A), delayed(np.sum)(B)

# Create another task to add the two sums
total = delayed(lambda x, y: x + y)(sumA, sumB)
result = total.compute()
print(result)
```

The two `np.sum` operations run concurrently on separate workers, then the final addition combines the results. 

**_Exercise 1._**

---
layout: cover

---
## Coffee Break
### 10:00 - 10:15 AM

---
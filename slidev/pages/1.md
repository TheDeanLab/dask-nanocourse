## Big Data Challenges

<v-clicks>

- **Data Volume:** Modern datasets (microscopy images, genomics); often exceed a single machine’s RAM, making traditional in-memory tools (e.g., Pandas); impractical. Even data close to memory size can be unwieldy due to copies during processing.
- **CPU Limitations:** Python’s Global Interpreter Lock (GIL) means pure Python code runs on only one core, so typical NumPy/Pandas workflows run single-threaded and fail to use modern multi-core CPUs. This leaves significant performance potential untapped on multi-core machines.
- **Need for Parallelism:** To handle big data within reasonable time, we must go _out-of-core_ (process data in chunks from disk) and exploit parallel computing—using all CPU cores or multiple cluster nodes in tandem. This requires tools beyond the standard single-threaded Python approach (e.g., multi-processing or distributed computing frameworks).

</v-clicks>

---

## Dask Overview - Out-of-Core & Lazy Execution

<v-clicks>

- **Blocked Algorithms**: Dask breaks up large data into many small pieces (chunks) that fit in memory. This enables computations on datasets larger than RAM by processing each chunk sequentially or in parallel (out-of-core execution). 
- **Lazy Evaluation**: Operations on Dask collections are _lazy_ – they build a task graph of work to be done, but no actual computation happens until you explicitly call `.compute()` or similar to execute the graph. This avoids needless work and allows optimization before running tasks.
- **Task Graph & Scheduler**: Each Dask computation creates a directed acyclic graph (DAG) of tasks. When triggered, Dask’s _scheduler_ orchestrates executing these tasks on parallel hardware (threads, processes, or cluster workers) to produce the result. This separation of the task graph from execution allows Dask to scale transparently.

_Dask execution flow_: User Collections (e.g. Dask DataFrame) create a Task Graph of many small tasks, which a _Scheduler_ executes on available resources. This design lets users write high-level code while Dask handles parallel execution. (Dask offers both a single-machine scheduler and a distributed scheduler for clusters.)

</v-clicks>

---

## Dask Collections (Parallel Data Structures)

Dask provides high-level collections that mimic familiar APIs, allowing you to scale up workflows with minimal code changes:

<v-clicks>

- Dask Array: Parallel N-dimensional array (NumPy-like). Useful for large numerical data (e.g. large images, multi-dim arrays) that don’t fit in memory. It uses all cores and can handle out-of-core arrays by operating on chunked blocks ￼. 
- Dask DataFrame: Parallel DataFrame (Pandas-like). For large tabular datasets (e.g. CSVs, data frames) split into many partitions. Supports a subset of the Pandas API, executed on chunks of the data in parallel, then aggregated.- 

</v-clicks>

---

## Dask Collections (Parallel Data Structures)

Dask provides high-level collections that mimic familiar APIs, allowing you to scale up workflows with minimal code changes:

<v-clicks>

- Dask Bag: Parallel collection for unstructured data (like a list of Python objects, text logs, JSON records). It’s a general-purpose container for data that isn’t easily represented in tables or arrays. Allows mapping/filtering Python functions over large datasets in parallel. 
- Dask Delayed: A low-level way to build custom task graphs. You wrap arbitrary Python functions with dask.delayed to create tasks and dependencies manually. Use this for parallelizing code that doesn’t naturally fit into array or dataframe abstractions (we’ll see an example with delayed in the demo).

</v-clicks>

---

## Dask Collections (Parallel Data Structures)

Dask provides high-level collections that mimic familiar APIs, allowing you to scale up workflows with minimal code changes:

<v-clicks>

- All these collections run on the same Dask scheduler infrastructure. 
- They mirror NumPy/Pandas APIs closely, so you can often switch to Dask by changing imports and adding a `.compute()` call.

</v-clicks>

---

## Minimal Code Changes: Pandas vs Dask DataFrame

For example, switching from Pandas to Dask DataFrame in an analysis is straightforward ￼ – usually just change the import, possibly load data in partitions, and call .compute() when needed:

```python
# Pandas usage (single-machine, in-memory)
import pandas as pd
df = pd.read_csv("data.csv")
result = df.groupby('category')['value'].mean()

# Dask usage (parallel, can be larger-than-memory)
import dask.dataframe as dd
df = dd.read_csv("data_*.csv")        # can read multiple files in chunks
result = df.groupby('category')['value'].mean().compute()
```

Both snippets above do the same group-by operation. The Dask version spreads the work across many partitions and CPUs, then uses .compute() to get the final result.

---

## Minimal Code Changes: NumPy vs Dask Array
For NumPy arrays, switching to Dask Array is similarly straightforward:

```python
# NumPy usage (single-machine, in-memory)
import numpy as np
arr = np.random.rand(10000, 10000)  # large array
result = np.mean(arr, axis=0)
# Dask usage (parallel, can be larger-than-memory)
import dask.array as da
arr = da.random.random((10000, 10000), chunks=(1000, 1000))  # large array in chunks
result = da.mean(arr, axis=0).compute()  # compute mean across chunks
```

Both snippets perform the same mean operation on a large array. The Dask version processes the data in chunks, allowing it to handle arrays larger than memory and utilize all CPU cores.

---

## Parallelism on a Single Machine

<v-clicks>

- Multi-Threading: Dask’s default scheduler on a single machine uses a thread pool to execute tasks concurrently. This incurs very little overhead (tasks run in the same process), and works well when computations release the GIL (e.g. NumPy ufuncs, Pandas C code)
  - However, due to Python’s GIL, pure Python code cannot run on multiple threads at the same time, so multi-threading yields speedups mainly for numeric or I/O-bound workloads. 
- Multi-Processing: For Python-heavy workloads, Dask can use multiple processes. The multiprocessing scheduler runs tasks in separate Python processes, bypassing the GIL to achieve true parallelism. This allows parallel execution even for code that doesn’t release the GIL (e.g. processing text or Python objects). The trade-off is overhead: transferring data between processes can be costly, especially if large data needs to be serialized. 
- Choosing a Scheduler: Dask chooses a sensible default (threads for arrays/dataframes, processes for bags) but you can configure the scheduler. For compute-heavy pure Python tasks, using processes (or the distributed scheduler in local mode) can improve performance. For numeric array/dataframe tasks, threads are often effective since NumPy/Pandas do the heavy lifting in C/C++ (no GIL contention).

</v-clicks>

---

## Dask Distributed Scheduler (Clusters)

<v-clicks>

- Advanced Local Scheduler: Dask’s distributed scheduler can run on a single machine or across a cluster. Even on one machine, many users prefer the distributed scheduler for its richer features: an asynchronous Future API, a live diagnostic dashboard, and smarter task scheduling (e.g. data locality awareness) that can outperform the simple multiprocessing approach. You can start a local distributed scheduler with a few lines of code and get insight into task execution in real time.
- Scaling to Clusters: The distributed scheduler truly shines when scaling out to multiple machines. It can coordinate a cluster of worker processes spread across several nodes, handling communication and load balancing. Dask can integrate with HPC job schedulers (SLURM, PBS, LSF, etc.) to launch workers on a cluster. This means you can write your code using Dask’s API and then deploy it on an HPC cluster, harnessing hundreds of cores or more.
- Same Code, Larger Scale: Thanks to Dask’s abstraction, code written for Dask on your laptop can run on a cluster with minimal or no changes. The scheduler and cluster take care of distributing data and computations. This lets you prototype locally and then scale out to big data on HPC when needed – a key benefit for scientific computing workflows.

</v-clicks>
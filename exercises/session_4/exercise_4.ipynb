{
 "cells": [
  {
   "cell_type": "code",
   "id": "1a111b209c9c3761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:57:29.845125Z",
     "start_time": "2025-08-03T12:57:24.489955Z"
    }
   },
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Third Party Imports\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "from dask import array as da\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "# Local Imports\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Specify File System\n",
    "- Run `umask` to check our default file-creation permissions.\n",
    "- Create a fully writable `local_directory` for Zarr’s intermediate chunks (required by `map_overlap`).\n",
    "- Define `data_path` for the input Zarr dataset and `save_path` for the output Zarr.\n",
    "\n",
    "**What is `umask`?**\n",
    "\n",
    "The “user file-creation mode mask” that determines the default permissions for **new** files and directories.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "  1. The system starts with a _full_ default permission (octal):\n",
    "     - **Files:** `666` (read/write for owner/group/others)\n",
    "     - **Dirs:**  `777` (read/write/execute for owner/group/others)\n",
    "  2. It then **subtracts** (bitwise) the umask value.\n",
    "     - Example: umask `022` → new files get `666 − 022 = 644` (`rw-r‐-r‐-`)\n",
    "                 new dirs get `777 − 022 = 755` (`rwx-r-xr-x`)\n",
    "\n",
    "- **Viewing your mask**\n",
    "  ```bash\n",
    "  $ umask\n",
    "  0022"
   ],
   "id": "79ad768c87dc1b9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:57:31.903716Z",
     "start_time": "2025-08-03T12:57:31.790727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Figure out what your default `umask` setting is.\n",
    "result = subprocess.run(\"umask\", shell=True, capture_output=True, text=True)\n",
    "print(\"Subprocess umask:\", result.stdout.strip())\n",
    "\n",
    "# Create temporary directory and make sure that we have write privileges.\n",
    "local_directory=\"/project/bioinformatics/Danuser_lab/Dean/dean/dask_temp\"\n",
    "subprocess.run(f\"mkdir -p {local_directory} && chmod -R 777 {local_directory}\", shell=True)\n",
    "\n",
    "# Location of the data.\n",
    "base_path = \"/archive/bioinformatics/Danuser_lab/Dean/dean/2024-05-21-tiling\"\n",
    "data_path = os.path.join(base_path, \"cell5_fused_tp_0_ch_0.zarr\")\n",
    "save_path = os.path.join(base_path, 'example_4.zarr')\n"
   ],
   "id": "d403376cb17050e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess umask: 0022\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Specify your cluster's operating parameters.",
   "id": "2123813d39335372"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:57:35.301192Z",
     "start_time": "2025-08-03T12:57:35.289121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_kwargs = {\n",
    "    'cores': 28, # Number of threads per worker (utilizing cores within each process)\n",
    "    'processes': 1, # Number of Python processes/worker.\n",
    "    'memory': '220GB', # Total memory to allocate for each worker job.\n",
    "    'local_directory': local_directory, #  Path for the Dask worker’s local storage (scratch space).\n",
    "    'interface': 'ib0', # Network interface identifier for Dask communications. Infiniband.\n",
    "    'walltime': \"01:00:00\", # The wall-time limit for each job, in HH:MM:SS.\n",
    "    'job_name': \"nanocourse\", # Name for the Slurm job, publicly visible via squeue command.\n",
    "    'queue': \"256GB\", # Slurm partition/queue to submit the jobs to\n",
    "    'death_timeout': \"600s\", #  Timeout (in seconds) for worker survival without a scheduler connection.\n",
    "    'job_extra_directives': [\n",
    "        # --nodes=1 and --ntasks=1 ensure each job runs on a single node with one task\n",
    "        \"--nodes=1\",\n",
    "        \"--ntasks=1\",\n",
    "        \"--mail-type=FAIL\",\n",
    "        \"--mail-user=kevin.dean@utsouthwestern.edu\",\n",
    "        \"-o job_%j.out\",\n",
    "        \"-e job_%j.err\",\n",
    "    ],\n",
    "    'scheduler_options': {\n",
    "        # A dictionary of settings passed to the Dask scheduler.\n",
    "        \"dashboard_address\": \":9000\",       # Dashboard web interface port\n",
    "        \"interface\": \"ib0\",\n",
    "\n",
    "        # Resource management\n",
    "        \"idle_timeout\": \"3600s\",            # How long workers stay alive when idle (1 hour)\n",
    "        \"allowed_failures\": 10,             # More failures allowed before worker marked as bad\n",
    "    },\n",
    "}\n"
   ],
   "id": "ccb6939dbcea1dc3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Important `cluster_kwargs`\n",
    "\n",
    "-  cores: 8 – Total number of CPU cores allocated per Dask worker job. With processes=1, this means the single worker process will use 8 threads (8 cores) for parallel computations ￼. This value is also used by Slurm to request 8 CPUs for the job (effectively --cpus-per-task=8 when combined with one task).\n",
    "-  processes: 1 – Number of separate Python worker processes to start per job. Here 1 process will utilize all the threads/cores in the job. Using a single process is common if your tasks release the GIL or benefit from multi-threading; if tasks were pure Python (GIL-bound) or the node had many cores, you might increase this to spawn multiple smaller processes ￼ (each with cores/processes threads).\n"
   ],
   "id": "dc51d025090a03b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions for Launching the Operation",
   "id": "d61d86b000840b86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:05:01.964093Z",
     "start_time": "2025-08-03T13:05:01.943249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wait_for_cluster(client, number_of_workers, start_time, timeout=120):\n",
    "    print(\"Waiting for workers to connect\", end=\"\", flush=True)\n",
    "\n",
    "    prev_count = 0\n",
    "    while True:\n",
    "        curr_count = len(client.scheduler_info()['workers'])\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(\"\\nTimed out. Canceling.\")\n",
    "            break\n",
    "\n",
    "        # if no workers yet, print a dot (once) and continue\n",
    "        if curr_count == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            prev_count = 0\n",
    "            time.sleep(0.3)\n",
    "            continue\n",
    "\n",
    "        if curr_count != prev_count:\n",
    "            print(f\"{curr_count} workers are connected.\", flush=True)\n",
    "            prev_count = curr_count\n",
    "\n",
    "        if curr_count >= number_of_workers:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "def launch_job(data_path, save_path, number_of_workers, cluster_kwargs):\n",
    "\n",
    "    cluster = SLURMCluster(**cluster_kwargs)\n",
    "    cluster.scale(number_of_workers+1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    start_time = time.time()\n",
    "    wait_for_cluster(client, number_of_workers, start_time, timeout=120)\n",
    "    print(f\"Client dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "    # Load the Zarr file with Dask\n",
    "    dask_data = da.from_zarr(data_path, component='0/0')\n",
    "    data_shape = dask_data.shape\n",
    "\n",
    "    # Eliminate singleton dimensions, and rechunk the data.\n",
    "    dask_data = dask_data.squeeze()\n",
    "    dask_data = dask_data.rechunk((32, 64, 64))\n",
    "\n",
    "    # Process the data\n",
    "    high_pass_filtered = dask_data.map_overlap(\n",
    "        ndimage.gaussian_filter, sigma=3, order=0, mode=\"nearest\", depth=40)\n",
    "\n",
    "    low_pass_filtered = dask_data.map_overlap(\n",
    "        ndimage.gaussian_filter, sigma=10, order=0, mode=\"nearest\", depth=40)\n",
    "\n",
    "    dog_filtered = da.map_blocks(\n",
    "        np.subtract, high_pass_filtered, low_pass_filtered)\n",
    "\n",
    "    dog_filtered.to_zarr(save_path, overwrite=True)\n",
    "\n",
    "    # Close the client and cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    print(\"Client and cluster closed.\")\n",
    "    print(f\"Total time to compute: {time.time() - start_time}\")\n"
   ],
   "id": "408949e108bfa50",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Port Forwarding\n",
    "Dask will let you know which port the HTTP server is operating on. For example, it will state:\n",
    "`Hosting the HTTP server on port 44460 instead`. The address can be found from `client.dashboard_link`.\n",
    "\n",
    "To forward it to your local machine, run the following ssh command in your Terminal.\n",
    "`ssh -N -L 44460:localhost:44460 your-cluster-login@nucleus.biohpc.swmed.edu`\n",
    "\n",
    "You can then access this at `http://localhost:44460/status`\n"
   ],
   "id": "f0bd87da6a78b39c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:10:02.193578Z",
     "start_time": "2025-08-03T13:05:25.428324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "number_of_workers = 4\n",
    "launch_job(data_path, save_path, number_of_workers, cluster_kwargs)"
   ],
   "id": "8c6090664ee81bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for workers to connect................\n",
      "3 workers are connected.\n",
      "\n",
      "5 workers are connected.\n",
      "Client dashboard available at: http://10.100.160.4:9000/status\n",
      "Client and cluster closed.\n",
      "Total time to compute: 276.55252408981323\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimization\n",
   "id": "215997aa8f72e3f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:20:06.236222Z",
     "start_time": "2025-08-03T13:14:30.658933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "number_of_workers = 8\n",
    "launch_job(data_path, save_path, number_of_workers, cluster_kwargs)"
   ],
   "id": "796871c1be17611f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for workers to connect................\n",
      "5 workers are connected.\n",
      "\n",
      "Timed out. Canceling.\n",
      "Client dashboard available at: http://10.100.160.4:9000/status\n",
      "Client and cluster closed.\n",
      "Total time to compute: 335.36883425712585\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimization\n",
    "With 8 workers, and 28 cores per worker, it took 336 seconds to compute. What gives?\n",
    "That’s probably a sign of overhead—extra scheduler traffic, more contention for the shared filesystem, and greater serialization costs on each map_overlap boundary."
   ],
   "id": "d0299bdd01b7131f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb986e7f16f7dcd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
